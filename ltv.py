# -*- coding: utf-8 -*-
"""LTV.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12L9Uht9TfB5F0btFUxcmGOI-3azDj7PD
"""

# ==========================
# IMPORTS
# ==========================
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor
from sklearn.metrics import classification_report, mean_squared_error, r2_score, roc_auc_score

# ==========================
# IMPORTS
# ==========================
import pandas as pd
import numpy as np
from datetime import timedelta
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor
from sklearn.linear_model import Ridge
from sklearn.metrics import classification_report, mean_squared_error, r2_score, roc_auc_score
from google.colab import files

# ==========================
# UPLOAD DATA
# ==========================
uploaded = files.upload()  # Select data.csv
df = pd.read_csv('data.csv')
df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])
df['Sales'] = df['Quantity'] * df['UnitPrice']

# ==========================
# PHASE 1: CHURN CLASSIFICATION
# ==========================
SPLIT_DATE = df['InvoiceDate'].max() - timedelta(days=180)
CHURN_WINDOW = 30  # Days of inactivity defines churn

df_obs = df[df['InvoiceDate'] < SPLIT_DATE].copy()
df_pred = df[df['InvoiceDate'] >= SPLIT_DATE].copy()

if df_obs.empty:
    raise ValueError("Observation DataFrame is empty. Adjust SPLIT_DATE further back.")

# --- RFM Features ---
SNAPSHOT_DATE_OBS = df_obs['InvoiceDate'].max() + timedelta(days=1)
CHURN_PERIOD = SNAPSHOT_DATE_OBS - timedelta(days=CHURN_WINDOW)

rfm_obs = df_obs.groupby('CustomerID').agg(
    Recency=('InvoiceDate', lambda x: (SNAPSHOT_DATE_OBS - x.max()).days),
    Frequency=('InvoiceDate', 'nunique'),
    Monetary=('Sales', 'sum')
).reset_index()

last_purchase_obs = df_obs.groupby('CustomerID')['InvoiceDate'].max()
rfm_obs['Churn'] = (last_purchase_obs < CHURN_PERIOD).fillna(True).astype(int)

# --- Future LTV Target ---
ltv_pred = df_pred.groupby('CustomerID')['Sales'].sum().reset_index()
ltv_pred.rename(columns={'Sales':'Future_LTV'}, inplace=True)
final_df = rfm_obs.merge(ltv_pred, on='CustomerID', how='left').fillna(0)

# --- Scaling ---
X = final_df[['Recency','Frequency','Monetary']]
y_churn = final_df['Churn']
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# --- Train classifier only if both classes exist ---
if len(np.unique(y_churn)) < 2:
    print("Only one class present. Skipping churn classification.")
    final_df['Predicted_Churn'] = 0
else:
    X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(
        X_scaled, y_churn, test_size=0.3, random_state=42, stratify=y_churn
    )
    gbi_classifier = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1,
                                                max_depth=3, random_state=42)
    gbi_classifier.fit(X_train_c, y_train_c)
    y_pred_c = gbi_classifier.predict(X_test_c)

    print("="*50)
    print("PHASE 1: CHURN CLASSIFICATION REPORT")
    print("="*50)
    print(classification_report(y_test_c, y_pred_c, zero_division=0))

    final_df['Predicted_Churn'] = gbi_classifier.predict(X_scaled)

    # --- Compute AUC ---
    y_pred_proba = gbi_classifier.predict_proba(X_test_c)[:,1]
    auc = roc_auc_score(y_test_c, y_pred_proba)
    print(f"Churn Model AUC: {auc*100:.2f}%")

# ==========================
# PHASE 2: LTV REGRESSION (Log + Gradient Boosting)
# ==========================
active_df = final_df[final_df['Predicted_Churn']==0].copy()

if active_df.empty:
    print("No active customers predicted. Skipping LTV regression.")
else:
    # Log-transform target
    active_df['Future_LTV_Log'] = np.log1p(active_df['Future_LTV'])

    X_reg = scaler.transform(active_df[['Recency','Frequency','Monetary']])
    y_ltv_log = active_df['Future_LTV_Log']

    X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(
        X_reg, y_ltv_log, test_size=0.3, random_state=42
    )

    gbr = GradientBoostingRegressor(n_estimators=300, learning_rate=0.05,
                                    max_depth=3, subsample=0.9, random_state=42)
    gbr.fit(X_train_r, y_train_r)

    y_pred_log = gbr.predict(X_test_r)
    rmse_log = np.sqrt(mean_squared_error(y_test_r, y_pred_log))
    r2_log = r2_score(y_test_r, y_pred_log)

    y_pred_real = np.expm1(y_pred_log)
    y_true_real = np.expm1(y_test_r)
    rmse_real = np.sqrt(mean_squared_error(y_true_real, y_pred_real))
    r2_real = r2_score(y_true_real, y_pred_real)

    print("="*50)
    print("PHASE 2: LTV REGRESSION REPORT")
    print("="*50)
    print(f"Model: Gradient Boosting Regressor (Log-transformed target)")
    print(f"Features: Recency, Frequency, Monetary (scaled)")
    print(f"Target: Future LTV (Sales after {SPLIT_DATE.date()})")
    print(f"RMSE (Real $): ${rmse_real:,.2f}")
    print(f"R2 Score (Real): {r2_real:.4f}")
    print(f"R2 Score (Log-space): {r2_log:.4f}")
    print("="*50)